# Modern LLM Architectures
*Clean, Educational PyTorch Implementations of State-of-the-Art Language Models*

## Project Vision

This project provides clean, readable PyTorch implementations of modern large language model architectures with a focus on **learning and understanding**. Inspired by Sebastian Raschka's excellent blog post ["The Big LLM Architecture Comparison"](https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison), I aim to create a hands-on resource that makes cutting-edge architectures accessible to researchers, students, and practitioners.

## Why This Project?

- **Readability First**: Every implementation prioritizes clarity over performance optimization
- **Verified Correctness**: All implementations tested for numerical equivalence with HuggingFace models
- **Modern Architectures**: Coverage of the latest innovations in transformer design
- **Easy Navigation**: Consistent structure across all model implementations

## Supported Architectures

### Planned Models (Roadmap)

| Model | Status | Key Innovations |
|-------|--------|----------------|
| **Gemma 3** | Planned | local/global attn, Normalizer location, GeGLU |
| **Llama 4** | In Progress | RMSNorm, SwiGLU, RoPE |
| **Qwen 3** | Planned | Dual chunk attention |
| **DeepSeek V3/R1** | Planned | Multi-head latent attention, MoE |
| **OLMo 2** | Planned | Academic transparency focus |
| **Mistral Small 3.1** | Planned | Sliding window attention |
| **Qwen 3-Next** | Planned | Next-generation improvements |
| **SmolLM 3** | Planned | Efficiency optimizations |
| **Kimi 2** | Planned | Long context handling |
| **GPT-OSS** | Planned | Open-source GPT variant |
| **Grok 2.5** | Planned | xAI innovations |
| **GLM-4.5** | Planned | ChatGLM improvements |
